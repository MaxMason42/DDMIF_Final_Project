{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the papers:\n",
    "\n",
    "@article{wood2021trading,\n",
    "  title={Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture},\n",
    "  author={Wood, Kieran and Giegerich, Sven and Roberts, Stephen and Zohren, Stefan},\n",
    "  journal={arXiv preprint arXiv:2112.08534},\n",
    "  year={2021}\n",
    "}\n",
    "\n",
    "@article {Wood111,\n",
    "\tauthor = {Wood, Kieran and Roberts, Stephen and Zohren, Stefan},\n",
    "\ttitle = {Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection},\n",
    "\tvolume = {4},\n",
    "\tnumber = {1},\n",
    "\tpages = {111--129},\n",
    "\tyear = {2022},\n",
    "\tdoi = {10.3905/jfds.2021.1.081},\n",
    "\tpublisher = {Institutional Investor Journals Umbrella},\n",
    "\tissn = {2640-3943},\n",
    "\tURL = {https://jfds.pm-research.com/content/4/1/111},\n",
    "\teprint = {https://jfds.pm-research.com/content/4/1/111.full.pdf},\n",
    "\tjournal = {The Journal of Financial Data Science}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached gpflow-2.9.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting check-shapes>=1.0.0 (from gpflow)\n",
      "  Using cached check_shapes-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting deprecated (from gpflow)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting multipledispatch>=0.6 (from gpflow)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpflow) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpflow) (23.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpflow) (1.12.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpflow) (65.5.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpflow) (0.9.0)\n",
      "Collecting tensorflow-probability>=0.12.0 (from tensorflow-probability[tf]>=0.12.0->gpflow)\n",
      "  Using cached tensorflow_probability-0.25.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\maxim\\appdata\\roaming\\python\\python310\\site-packages (from gpflow) (4.12.2)\n",
      "Requirement already satisfied: tensorflow>=2.4.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpflow) (2.8.1)\n",
      "Collecting dropstackframe>=0.1.0 (from check-shapes>=1.0.0->gpflow)\n",
      "  Using cached dropstackframe-0.1.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting lark<2.0.0,>=1.1.0 (from check-shapes>=1.0.0->gpflow)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (24.3.25)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (3.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (3.17.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\maxim\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow>=2.4.0->gpflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (1.17.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (2.8.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow>=2.4.0->gpflow) (1.68.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\maxim\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-probability>=0.12.0->tensorflow-probability[tf]>=0.12.0->gpflow) (5.1.1)\n",
      "Collecting cloudpickle>=1.3 (from tensorflow-probability>=0.12.0->tensorflow-probability[tf]>=0.12.0->gpflow)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting dm-tree (from tensorflow-probability>=0.12.0->tensorflow-probability[tf]>=0.12.0->gpflow)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting tensorflow>=2.4.0 (from gpflow)\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tf-keras>=2.16 (from tensorflow-probability[tf]>=0.12.0->gpflow)\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow>=2.4.0->gpflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow)\n",
      "  Downloading protobuf-5.29.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (2.28.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (3.7)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\maxim\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.4.0->gpflow) (0.1.2)\n",
      "Using cached gpflow-2.9.2-py3-none-any.whl (392 kB)\n",
      "Using cached check_shapes-1.1.1-py3-none-any.whl (45 kB)\n",
      "Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Using cached tensorflow_probability-0.25.0-py2.py3-none-any.whl (7.0 MB)\n",
      "Downloading tensorflow-2.18.0-cp310-cp310-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp310-cp310-win_amd64.whl (390.0 MB)\n",
      "   ---------------------------------------- 0.0/390.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 8.4/390.0 MB 47.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 22.0/390.0 MB 55.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 38.0/390.0 MB 63.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 53.7/390.0 MB 65.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 70.0/390.0 MB 68.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 88.1/390.0 MB 71.2 MB/s eta 0:00:05\n",
      "   ---------- ---------------------------- 104.6/390.0 MB 72.6 MB/s eta 0:00:04\n",
      "   ----------- --------------------------- 119.5/390.0 MB 71.3 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 132.6/390.0 MB 70.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 147.1/390.0 MB 70.1 MB/s eta 0:00:04\n",
      "   ---------------- ---------------------- 161.2/390.0 MB 69.6 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 174.6/390.0 MB 68.9 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 188.2/390.0 MB 68.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 202.9/390.0 MB 68.6 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 215.5/390.0 MB 68.2 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 229.1/390.0 MB 68.1 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 248.3/390.0 MB 69.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 266.1/390.0 MB 71.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 284.4/390.0 MB 72.5 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 302.5/390.0 MB 73.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 320.6/390.0 MB 73.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 335.8/390.0 MB 73.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 350.7/390.0 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 367.0/390.0 MB 72.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  380.4/390.0 MB 72.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 72.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 72.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.0/390.0 MB 63.7 MB/s eta 0:00:00\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Using cached dropstackframe-0.1.1-py3-none-any.whl (4.6 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "Downloading dm_tree-0.1.8-cp310-cp310-win_amd64.whl (101 kB)\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 60.4 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.1-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: multipledispatch, dm-tree, tensorboard-data-server, protobuf, lark, dropstackframe, deprecated, cloudpickle, tensorflow-probability, tensorboard, check-shapes, keras, tensorflow-intel, tensorflow, tf-keras, gpflow\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.17.3\n",
      "    Uninstalling protobuf-3.17.3:\n",
      "      Successfully uninstalled protobuf-3.17.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.8.0\n",
      "    Uninstalling keras-2.8.0:\n",
      "      Successfully uninstalled keras-2.8.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.8.1\n",
      "    Uninstalling tensorflow-2.8.1:\n",
      "      Successfully uninstalled tensorflow-2.8.1\n",
      "Successfully installed check-shapes-1.1.1 cloudpickle-3.1.0 deprecated-1.2.15 dm-tree-0.1.8 dropstackframe-0.1.1 gpflow-2.9.2 keras-3.7.0 lark-1.2.2 multipledispatch-1.0.0 protobuf-5.29.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-probability-0.25.0 tf-keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gpflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'imp'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "from gpflow.kernels import ChangePoints, Matern32\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Kernel = gpflow.kernels.base.Kernel\n",
    "MAX_ITERATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangePointDetection(ChangePoints):\n",
    "    def __init__(\n",
    "            self,\n",
    "            kernels: Tuple[Kernel, Kernel],\n",
    "            location: float,\n",
    "            interval: Tuple[float, float],\n",
    "            steepness: float = 1.0,\n",
    "            name: Optional[str] = None\n",
    "    ):\n",
    "        if location < interval[0] or location > interval[1]:\n",
    "            raise ValueError(\n",
    "                \"Location {loc} is not in range [{low},{high}]\".format(\n",
    "                    loc=location, low=interval[0], high=interval[1]\n",
    "                )\n",
    "            )\n",
    "        locations = [location]\n",
    "        super().__init__(\n",
    "            kernels = kernels, locations = locations, steepness = steepness, name=name\n",
    "        )\n",
    "\n",
    "        affine = tfb.Shift(tf.cast(interval[0], tf.float64))(\n",
    "            tfb.Scale(tf.cast(interval[1] - interval[0], tf.float64))\n",
    "        )\n",
    "\n",
    "        self.locations = gpflow.Parameter(\n",
    "            locations, transform=tfb.Chain([affine, tfb.Sigmoid()]), dtype = tf.float64\n",
    "        )\n",
    "\n",
    "        def _sigmoids(self, X: tf.Tensor):\n",
    "            locations = tf.reshape(self.locations, (1, 1, -1))\n",
    "            steepness = tf.reshape(self.steepness, (1, 1, -1))\n",
    "            return tf.sigmoid(steepness * (X[:, :, None] - locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_matern_kernel(\n",
    "        time_series_data: pd.DataFrame,\n",
    "        variance: float = 1.0,\n",
    "        lengthscale: float = 1.0,\n",
    "        likelihood_variance: float = 1.0\n",
    "):\n",
    "    \n",
    "    model = gpflow.models.GPR(\n",
    "        data = (\n",
    "            time_series_data.loc[:, [\"X\"]].to_numpy(),\n",
    "            time_series_data.loc[:, [\"Y\"]].to_numpy()\n",
    "        ),\n",
    "        kernel = Matern32(variance=variance, lengthscales=lengthscale),\n",
    "        noise_variance=likelihood_variance\n",
    "    )\n",
    "\n",
    "    optimizer = gpflow.optimizers.Scipy()\n",
    "    nlml = optimizer.minimize(\n",
    "        model.training_loss, model.trainable_variables, options=dict(maxiter=MAX_ITERATIONS)\n",
    "    ).fun\n",
    "    parameters = {\n",
    "        \"kM_variance\": model.kernel.variance.numpy(),\n",
    "        \"kM_lengthscales\": model.kernel.lengthscales.numpy(),\n",
    "        \"kM_likelihood_variance\": model.likelihood.variance.numpy()\n",
    "    }\n",
    "\n",
    "    return nlml, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_changepoint_kernel(\n",
    "        time_series_data: pd.DataFrame,\n",
    "        k1_variance: float = 1.0,\n",
    "        k1_lengthscale: float = 1.0,\n",
    "        k2_variance: float = 1.0,\n",
    "        k2_lengthscale: float = 1.0,\n",
    "        kC_likelihood_variance = 1.0,\n",
    "        kC_changepoint_location = None,\n",
    "        kC_steepness = 1.0\n",
    "):\n",
    "    if not kC_changepoint_location:\n",
    "        kC_changepoint_location = (\n",
    "            time_series_data[\"X\"].iloc[0] + time_series_data[\"X\"].iloc[-1]\n",
    "        ) / 2.0\n",
    "\n",
    "    model = gpflow.models.GPR(\n",
    "        data=(\n",
    "            time_series_data.loc[:, [\"X\"]].to_numpy(),\n",
    "            time_series_data.loc[:, [\"Y\"]].to_numpy()\n",
    "        ),\n",
    "        kernel = ChangePointDetection(\n",
    "            [\n",
    "                Matern32(variance=k1_variance, lengthscales=k1_lengthscale),\n",
    "                Matern32(variance=k2_variance, lengthscales=k2_lengthscale)\n",
    "            ],\n",
    "            location=kC_changepoint_location,\n",
    "            interval=(time_series_data[\"X\"].iloc[0], time_series_data[\"X\"].iloc[-1]),\n",
    "            steepness=kC_steepness\n",
    "        )\n",
    "    )\n",
    "    model.likelihood.variance.assign(kC_likelihood_variance)\n",
    "    \n",
    "    optimizer = gpflow.optimizers.Scipy()\n",
    "    nlml = optimizer.minimize(\n",
    "        model.training_loss, model.trainable_variables, options=dict(maxiter=MAX_ITERATIONS)\n",
    "    ).fun\n",
    "    changepoint_location = model.kernel.locations[0].numpy()\n",
    "    parameters = {\n",
    "        \"k1_variance\": model.kernel.kernels[0].variance.numpy().flatten()[0],\n",
    "        \"k1_lengthscale\": model.kernel.kernels[0].lengthscales.numpy().flatten()[0],\n",
    "        \"k2_variance\": model.kernel.kernels[1].variance.numpy().flatten()[0],\n",
    "        \"k2_lengthscale\": model.kernel.kernels[1].lengthscales.numpy().flatten()[0],\n",
    "        \"kC_likelihood_variance\": model.likelihood.variance.numpy().flatten()[0],\n",
    "        \"kC_changepoint_location\": changepoint_location,\n",
    "        \"kC_steepness\": model.kernel.steepness.numpy()\n",
    "    }\n",
    "\n",
    "    return changepoint_location, nlml, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changepoint_severity(\n",
    "     kC_nlml: Union[float, List[float]], \n",
    "     kM_nlml: Union[float, List[float]]\n",
    "):\n",
    "    normalized_nlml = kC_nlml - kM_nlml\n",
    "    return 1 - 1 / (np.mean(np.exp(-normalized_nlml)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changepoint_loc_and_score(\n",
    "    time_series_data_window: pd.DataFrame,\n",
    "    kM_variance: float = 1.0,\n",
    "    kM_lengthscale: float = 1.0,\n",
    "    kM_likelihood_variance: float = 1.0,\n",
    "    k1_variance: float = None,\n",
    "    k1_lengthscale: float = None,\n",
    "    k2_variance: float = None,\n",
    "    k2_lengthscale: float = None,\n",
    "    kC_likelihood_variance: float = None,\n",
    "    kC_changepoint_location: float = None,\n",
    "    kC_steepness=1.0\n",
    "):\n",
    "    time_series_data = time_series_data_window.copy()\n",
    "    Y_data = time_series_data[[\"Y\"]].values\n",
    "    time_series_data[[\"Y\"]] = StandardScaler().fit(Y_data).transform(Y_data)\n",
    "\n",
    "    \n",
    "    if kM_variance == kM_lengthscale == kM_likelihood_variance == 1.0 :\n",
    "        (kM_nlml, kM_params) = fit_matern_kernel(time_series_data)\n",
    "    else:\n",
    "        (kM_nlml, kM_params) = fit_matern_kernel(time_series_data, kM_variance, kM_lengthscale, kM_likelihood_variance)\n",
    "    \n",
    "    is_cp_location_default = (\n",
    "        (not kC_changepoint_location)\n",
    "        or kC_changepoint_location < time_series_data[\"X\"].iloc[0]\n",
    "        or kC_changepoint_location > time_series_data[\"X\"].iloc[-1]\n",
    "    )\n",
    "\n",
    "    if is_cp_location_default:\n",
    "        kC_changepoint_location = (\n",
    "            time_series_data[\"X\"].iloc[-1] + time_series_data[\"X\"].iloc[0]\n",
    "        ) / 2.0\n",
    "\n",
    "    if not k1_variance:\n",
    "        k1_variance = kM_params[\"kM_variance\"]\n",
    "\n",
    "    if not k1_lengthscale:\n",
    "        k1_lengthscale = kM_params[\"kM_lengthscales\"]\n",
    "\n",
    "    if not k2_variance:\n",
    "        k2_variance = kM_params[\"kM_variance\"]\n",
    "\n",
    "    if not k2_lengthscale:\n",
    "        k2_lengthscale = kM_params[\"kM_lengthscales\"]\n",
    "\n",
    "    if not kC_likelihood_variance:\n",
    "        kC_likelihood_variance = kM_params[\"kM_likelihood_variance\"]\n",
    "\n",
    "\n",
    "    if (k1_variance == k1_lengthscale == k2_variance == k2_lengthscale == kC_likelihood_variance == kC_steepness == 1.0) and is_cp_location_default:\n",
    "        (changepoint_location, kC_nlml, kC_params) = fit_changepoint_kernel(time_series_data)\n",
    "    else:\n",
    "        (changepoint_location, kC_nlml, kC_params) = fit_changepoint_kernel(\n",
    "            time_series_data,\n",
    "            k1_variance=k1_variance,\n",
    "            k1_lengthscale=k1_lengthscale,\n",
    "            k2_variance=k2_variance,\n",
    "            k2_lengthscale=k2_lengthscale,\n",
    "            kC_likelihood_variance=kC_likelihood_variance,\n",
    "            kC_changepoint_location=kC_changepoint_location,\n",
    "            kC_steepness=kC_steepness,\n",
    "        )\n",
    "    \n",
    "    cp_score = changepoint_severity(kC_nlml, kM_nlml)\n",
    "    cp_loc_normalised = (time_series_data[\"X\"].iloc[-1] - changepoint_location) / (\n",
    "        time_series_data[\"X\"].iloc[-1] - time_series_data[\"X\"].iloc[0]\n",
    "    )\n",
    "\n",
    "    return cp_score, changepoint_location, cp_loc_normalised, kM_params, kC_params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CPD(\n",
    "    time_series_data: pd.DataFrame,\n",
    "    lookback_window_length: int,\n",
    "    start_date: dt.datetime = None,\n",
    "    end_date: dt.datetime = None,\n",
    "    use_kM_hyp_to_initialize_kC=True\n",
    "):\n",
    "    if start_date and end_date:\n",
    "        first_window = time_series_data.loc[:start_date].iloc[\n",
    "            -(lookback_window_length + 1) :, :\n",
    "        ]\n",
    "        remaining_data = time_series_data.loc[start_date:end_date, :]\n",
    "        if remaining_data.index[0] == start_date:\n",
    "            remaining_data = remaining_data.iloc[1:, :]\n",
    "        else:\n",
    "            first_window = first_window.iloc[1:]\n",
    "        time_series_data = pd.concat([first_window, remaining_data]).copy()\n",
    "    else:\n",
    "        raise Exception(\"Pass start and end date.\")\n",
    "\n",
    "    time_series_data[\"date\"] = time_series_data.index\n",
    "    time_series_data = time_series_data.reset_index(drop=True)\n",
    "\n",
    "    results = []\n",
    "    for window_end in range(lookback_window_length + 1, len(time_series_data)):\n",
    "        ts_data_window = time_series_data.iloc[\n",
    "            window_end - (lookback_window_length + 1) : window_end\n",
    "        ][[\"date\", \"daily_returns\"]].copy()\n",
    "        ts_data_window[\"X\"] = ts_data_window.index.astype(float)\n",
    "        ts_data_window = ts_data_window.rename(columns={\"daily_returns\": \"Y\"})\n",
    "        time_index = window_end - 1\n",
    "        window_date = ts_data_window[\"date\"].iloc[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if use_kM_hyp_to_initialize_kC:\n",
    "            cp_score, cp_loc, cp_loc_normalised, _, _ = changepoint_loc_and_score(ts_data_window)\n",
    "        else:\n",
    "            cp_score, cp_loc, cp_loc_normalised, _, _ = changepoint_loc_and_score(\n",
    "                    ts_data_window,\n",
    "                    k1_lengthscale=1.0,\n",
    "                    k1_variance=1.0,\n",
    "                    k2_lengthscale=1.0,\n",
    "                    k2_variance=1.0,\n",
    "                    kC_likelihood_variance=1.0,\n",
    "                )\n",
    "        results.append([window_date, time_index, cp_loc, cp_loc_normalised, cp_score]) \n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=[\"date\", \"t\", \"cp_location\", \"cp_location_norm\", \"cp_score\"]) \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n",
      "            daily_returns\n",
      "date                     \n",
      "2017-01-03       0.016437\n",
      "2017-01-04       0.009529\n",
      "2017-01-05       0.046053\n",
      "2017-01-06      -0.031447\n",
      "2017-01-09      -0.001694\n",
      "\n",
      "Data range: 2017-01-03 00:00:00 to 2023-12-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "\n",
    "#Change this to get the dates and companies that we are using\n",
    "\n",
    "\n",
    "# Step 1: Connect to WRDS\n",
    "conn = wrds.Connection()\n",
    "\n",
    "# Step 2: Define the ticker and date range\n",
    "ticker = 'NEM'\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Step 3: Query CRSP for the selected stock\n",
    "query = f\"\"\"\n",
    "    SELECT a.permno, a.date, a.ret, b.ticker \n",
    "    FROM crsp.dsf AS a\n",
    "    JOIN crsp.dsenames AS b \n",
    "    ON a.permno = b.permno \n",
    "    WHERE b.ticker = '{ticker}' \n",
    "    AND b.shrcd BETWEEN 10 AND 12\n",
    "    AND b.exchcd BETWEEN 1 AND 3\n",
    "    AND a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    AND a.date >= b.namedt\n",
    "    AND a.date <= b.nameendt\n",
    "\"\"\"\n",
    "\n",
    "# Fetch the data\n",
    "msft_data = conn.raw_sql(query)\n",
    "\n",
    "# Step 4: Process the data\n",
    "msft_data['date'] = pd.to_datetime(msft_data['date'])\n",
    "msft_data.set_index('date', inplace=True)\n",
    "msft_data = msft_data[['ret']].rename(columns={'ret': 'daily_returns'})\n",
    "\n",
    "# Step 5: Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Step 6: Display the data\n",
    "print(msft_data.head())\n",
    "print(\"\\nData range:\", msft_data.index.min(), \"to\", msft_data.index.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window_length = 21\n",
    "start_date = dt.datetime(2017, 1, 1)\n",
    "end_date = dt.datetime(2023, 12, 31)\n",
    "\n",
    "\n",
    "result = run_CPD(\n",
    "    time_series_data=msft_data,\n",
    "    lookback_window_length=lookback_window_length,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    use_kM_hyp_to_initialize_kC=True\n",
    ")\n",
    "\n",
    "result.to_csv('C:/Users/Maxim/Desktop/DDMIF/Changepoint files/nem_lbw21.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>t</th>\n",
       "      <th>cp_location</th>\n",
       "      <th>cp_location_norm</th>\n",
       "      <th>cp_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>21</td>\n",
       "      <td>16.641588</td>\n",
       "      <td>0.207543</td>\n",
       "      <td>0.878405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>22</td>\n",
       "      <td>15.236636</td>\n",
       "      <td>0.322065</td>\n",
       "      <td>0.865126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>23</td>\n",
       "      <td>18.081238</td>\n",
       "      <td>0.234227</td>\n",
       "      <td>0.954077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>24</td>\n",
       "      <td>18.049769</td>\n",
       "      <td>0.283344</td>\n",
       "      <td>0.991005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>25</td>\n",
       "      <td>18.165632</td>\n",
       "      <td>0.325446</td>\n",
       "      <td>0.999898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>1754</td>\n",
       "      <td>1748.195791</td>\n",
       "      <td>0.276391</td>\n",
       "      <td>0.926757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>1755</td>\n",
       "      <td>1748.933988</td>\n",
       "      <td>0.288858</td>\n",
       "      <td>0.951923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>1756</td>\n",
       "      <td>1748.513029</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.920551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>1757</td>\n",
       "      <td>1748.541038</td>\n",
       "      <td>0.402808</td>\n",
       "      <td>0.857886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>1758</td>\n",
       "      <td>1743.021627</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.690646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1738 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     t  cp_location  cp_location_norm  cp_score\n",
       "0     2017-02-02    21    16.641588          0.207543  0.878405\n",
       "1     2017-02-03    22    15.236636          0.322065  0.865126\n",
       "2     2017-02-06    23    18.081238          0.234227  0.954077\n",
       "3     2017-02-07    24    18.049769          0.283344  0.991005\n",
       "4     2017-02-08    25    18.165632          0.325446  0.999898\n",
       "...          ...   ...          ...               ...       ...\n",
       "1733  2023-12-21  1754  1748.195791          0.276391  0.926757\n",
       "1734  2023-12-22  1755  1748.933988          0.288858  0.951923\n",
       "1735  2023-12-26  1756  1748.513029          0.356522  0.920551\n",
       "1736  2023-12-27  1757  1748.541038          0.402808  0.857886\n",
       "1737  2023-12-28  1758  1743.021627          0.713256  0.690646\n",
       "\n",
       "[1738 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('C:/Users/Maxim/Desktop/DDMIF/Changepoint files/alco_lbw21.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
